{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq_dpu import DpuOverlay\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENCV_LOG_LEVEL\"]=\"SILENT\"\n",
    "from pynq.lib.video import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize displlay port\n",
    "displayport = DisplayPort()\n",
    "print(displayport.modes)\n",
    "displayport.configure(VideoMode(640, 480, 24), PIXEL_RGB) \n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.isOpened()\n",
    "\n",
    "capture.set(3, 640)\n",
    "capture.set(4, 480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download the DPU overlay onto the board.\n",
    "'''\n",
    "overlay = DpuOverlay(\"dpu.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_model()` method will automatically prepare the `graph`\n",
    "which is used by VART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Here we only load the feature generator and compression layers into the DPU.\n",
    "- Subsequent upsampling convolutional layers do not pass verification, so we move\n",
    "  those to post-processing on the CPU.\n",
    "- This hurts framerate, but gurantees correctness.\n",
    "'''\n",
    "overlay.load_model(\"backbone.xmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Image pre-processing function to prepare input data such \n",
    "  that it matches what was used during training.\n",
    "'''\n",
    "def rescale(image, output_size):\n",
    "    image_ = image/256.0\n",
    "    h, w = image_.shape[:2]\n",
    "    im_scale = min(float(output_size[0]) / float(h), float(output_size[1]) / float(w))\n",
    "    new_h = int(image_.shape[0] * im_scale)\n",
    "    new_w = int(image_.shape[1] * im_scale)\n",
    "    image = cv2.resize(image_, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    left_pad = (output_size[1] - new_w) // 2\n",
    "    right_pad = (output_size[1] - new_w) - left_pad\n",
    "    top_pad = (output_size[0] - new_h) // 2\n",
    "    bottom_pad = (output_size[0] - new_h) - top_pad\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    pad = ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "    image = np.stack([np.pad(image[:,:,c], pad, mode='constant', constant_values=mean[c])for c in range(3)], axis=2)\n",
    "    pose_fun = lambda x: ((((x.reshape([-1,2])+np.array([1.0,1.0]))/2.0*np.array(output_size)-[left_pad, top_pad]) * 1.0 /np.array([new_w, new_h])*np.array([w,h])))\n",
    "    return {'image': image, 'pose_fun': pose_fun}\n",
    "\n",
    "'''\n",
    "- Helper function to draw the pose wireframe onto the input image. Code from https://github.com/YuliangXiu/MobilePose.\n",
    "'''\n",
    "def draw_humans(npimg, pose, imgcopy=False):\n",
    "    if imgcopy:\n",
    "        npimg = np.copy(npimg)\n",
    "    image_h, image_w = npimg.shape[:2]\n",
    "    centers = {}\n",
    "\n",
    "    colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n",
    "          [170, 0, 255], [255, 0, 255]]\n",
    "\n",
    "    pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n",
    "    colors_skeleton = ['r', 'y', 'y', 'g', 'g', 'y', 'y', 'g', 'g', 'm', 'm', 'g', 'g', 'y','y']\n",
    "    colors_skeleton = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n",
    "          [170, 0, 255]]\n",
    "\n",
    "    for idx in range(len(colors)):\n",
    "        cv2.circle(npimg, (pose[idx,0], pose[idx,1]), 3, colors[idx], thickness=3, lineType=8, shift=0)\n",
    "    for idx in range(len(colors_skeleton)):\n",
    "        npimg = cv2.line(npimg, (pose[pairs[idx][0],0], pose[pairs[idx][0],1]), (pose[pairs[idx][1],0], pose[pairs[idx][1],1]), colors_skeleton[idx], 3)\n",
    "\n",
    "    return npimg\n",
    "\n",
    "'''\n",
    "- Helper function to crop camera inputs to a specified ratio. Code from https://github.com/YuliangXiu/MobilePose.\n",
    "'''\n",
    "def crop_camera(image, ratio=0.15):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    mid_width = width / 2.0\n",
    "    width_20 = width * ratio\n",
    "    crop_img = image[0:int(height), int(mid_width - width_20):int(mid_width + width_20)]\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "val, test_img = capture.read()\n",
    "\n",
    "# Convert to the expected RGB format\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Apply the rescale transform\n",
    "rescale_out = rescale(test_img, (224, 224))\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(rescale_out['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use VART\n",
    "Now we should be able to use VART API to do the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu = overlay.runner\n",
    "\n",
    "inputTensors = dpu.get_input_tensors()\n",
    "outputTensors = dpu.get_output_tensors()\n",
    "\n",
    "shapeIn = tuple(inputTensors[0].dims)\n",
    "shapeOut = tuple(outputTensors[0].dims)\n",
    "outputSize = int(outputTensors[0].get_data_size() / shapeIn[0])\n",
    "\n",
    "\n",
    "print(f\"Input Shape  : {shapeIn}\")\n",
    "print(f\"Output Shape : {shapeOut}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a few buffers to store input and output data.\n",
    "They will be reused during multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [np.empty(shapeOut, dtype=np.float32, order=\"C\")]\n",
    "input_data = [np.empty(shapeIn, dtype=np.float32, order=\"C\")]\n",
    "image = input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CPU (ONNX/FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 500\n",
    "\n",
    "ort_provider = ['CPUExecutionProvider']\n",
    "ort_sess = ort.InferenceSession('mobile_pose.onnx', providers=ort_provider)\n",
    "\n",
    "start = time()\n",
    "for _ in range(trials):\n",
    "    # Pre-processing\n",
    "    val, test_img = capture.read()\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB) #Conver to expected format\n",
    "   # test_img = cv2.imread('./test.jpg')\n",
    "   # test_img = crop_camera(test_img)\n",
    "    rescale_out = rescale(test_img, (224, 224))\n",
    "    rescaled_input = rescale_out['image'].astype(np.float32)\n",
    "    rescaled_input = np.swapaxes(rescaled_input, 0, 2)\n",
    "    rescaled_input = np.swapaxes(rescaled_input, 1, 2)\n",
    "    rescaled_input = np.expand_dims(rescaled_input, axis=0)\n",
    "\n",
    "    ort_inputs = {ort_sess.get_inputs()[0].name: rescaled_input}\n",
    "    coords = ort_sess.run(None, ort_inputs)[0]\n",
    "    coords = rescale_out['pose_fun'](coords).astype(int)\n",
    "    \n",
    "    #Display Output to monitor \n",
    "    res = draw_humans(test_img, coords)\n",
    "    res = cv2.cvtColor(res, cv2.COLOR_RGB2BGR)\n",
    "    #cv2.imwrite('inference_test.jpg', res)\n",
    "    frame = displayport.newframe()\n",
    "    frame[:] = res\n",
    "    displayport.writeframe(frame)\n",
    "stop = time()\n",
    "\n",
    "execution_time = stop - start\n",
    "print(\"Execution time: {:.4f}s\".format(execution_time))\n",
    "print(\"Throughput: {:.4f}FPS\".format(trials/execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CPU (ONNX/INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 500\n",
    "\n",
    "ort_provider = ['CPUExecutionProvider']\n",
    "ort_sess = ort.InferenceSession('mobile_pose_quant.onnx', providers=ort_provider)\n",
    "\n",
    "start = time()\n",
    "for _ in range(trials):\n",
    "    # Pre-processing\n",
    "    # Pre-processing\n",
    "    val, test_img = capture.read()\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB) #Conver to expected format\n",
    "   # test_img = cv2.imread('./test.jpg')\n",
    "   # test_img = crop_camera(test_img)\n",
    "    rescale_out = rescale(test_img, (224, 224))\n",
    "    rescaled_input = rescale_out['image'].astype(np.float32)\n",
    "    rescaled_input = np.swapaxes(rescaled_input, 0, 2)\n",
    "    rescaled_input = np.swapaxes(rescaled_input, 1, 2)\n",
    "    rescaled_input = np.expand_dims(rescaled_input, axis=0)\n",
    "\n",
    "    ort_inputs = {ort_sess.get_inputs()[0].name: rescaled_input}\n",
    "    coords = ort_sess.run(None, ort_inputs)[0]\n",
    "    coords = rescale_out['pose_fun'](coords).astype(int)\n",
    "    \n",
    "     #Display Output to monitor \n",
    "    res = draw_humans(test_img, coords)\n",
    "    res = cv2.cvtColor(res, cv2.COLOR_RGB2BGR)\n",
    "   # cv2.imwrite('inference_test.jpg', res)\n",
    "    frame = displayport.newframe()\n",
    "    frame[:] = res\n",
    "    displayport.writeframe(frame)\n",
    "stop = time()\n",
    "\n",
    "execution_time = stop - start\n",
    "print(\"Execution time: {:.4f}s\".format(execution_time))\n",
    "print(\"Throughput: {:.4f}FPS\".format(trials/execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DPU + CPU (ONNX/FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 500\n",
    "\n",
    "ort_provider = ['CPUExecutionProvider']\n",
    "ort_sess = ort.InferenceSession('post_proc.onnx', providers=ort_provider)\n",
    "\n",
    "start = time()\n",
    "for _ in range(trials):\n",
    "    # Pre-processing\n",
    "    val, test_img = capture.read()\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB) #Conver to expected forma\n",
    "    rescale_out = rescale(test_img, (224, 224))\n",
    "    rescaled_input = rescale_out['image']\n",
    "    \n",
    "    # Execute DPU inference\n",
    "    image[0,...] = rescaled_input\n",
    "    job_id = dpu.execute_async(input_data, output_data)\n",
    "    dpu.wait(job_id)\n",
    "    \n",
    "    # Extract the output from the DPU buffer and reshape it for further processing\n",
    "    x = np.array(output_data)[0]\n",
    "    x = np.swapaxes(x, 1, 3)\n",
    "    x = np.swapaxes(x, 2, 3)\n",
    "\n",
    "    # Post-processing\n",
    "    x = {ort_sess.get_inputs()[0].name: x}\n",
    "    coords = ort_sess.run(None, x)[0]\n",
    "    coords = rescale_out['pose_fun'](coords).astype(int)\n",
    "    \n",
    "    #Display Output to monitor \n",
    "    res = draw_humans(test_img, coords)\n",
    "    res = cv2.cvtColor(res, cv2.COLOR_RGB2BGR)\n",
    "    frame = displayport.newframe()\n",
    "    frame[:] = res\n",
    "    displayport.writeframe(frame)\n",
    "stop = time()\n",
    "\n",
    "execution_time = stop - start\n",
    "print(\"Execution time: {:.4f}s\".format(execution_time))\n",
    "print(\"Throughput: {:.4f}FPS\".format(trials/execution_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DPU + CPU (ONNX/INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 1000\n",
    "\n",
    "ort_provider = ['CPUExecutionProvider']\n",
    "ort_sess = ort.InferenceSession('post_proc_quant.onnx', providers=ort_provider)\n",
    "\n",
    "start = time()\n",
    "for _ in range(trials):\n",
    "    # Pre-processing\n",
    "    val, test_img = capture.read()\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB) #Conver to expected forma\n",
    "    rescale_out = rescale(test_img, (224, 224))\n",
    "    rescaled_input = rescale_out['image']\n",
    "    \n",
    "    # Execute DPU inference\n",
    "    image[0,...] = rescaled_input\n",
    "    job_id = dpu.execute_async(input_data, output_data)\n",
    "    dpu.wait(job_id)\n",
    "    \n",
    "    # Extract the output from the DPU buffer and reshape it for further processing\n",
    "    x = np.array(output_data)[0]\n",
    "    x = np.swapaxes(x, 1, 3)\n",
    "    x = np.swapaxes(x, 2, 3)\n",
    "\n",
    "    # Post-processing\n",
    "    x = {ort_sess.get_inputs()[0].name: x}\n",
    "    coords = ort_sess.run(None, x)[0]\n",
    "    coords = rescale_out['pose_fun'](coords).astype(int)\n",
    "    \n",
    "    #Display Output to monitor \n",
    "    res = draw_humans(test_img, coords)\n",
    "    res = cv2.cvtColor(res, cv2.COLOR_RGB2BGR)\n",
    "    frame = displayport.newframe()\n",
    "    frame[:] = res\n",
    "    displayport.writeframe(frame)\n",
    "stop = time()\n",
    "\n",
    "execution_time = stop - start\n",
    "print(\"Execution time: {:.4f}s\".format(execution_time))\n",
    "print(\"Throughput: {:.4f}FPS\".format(trials/execution_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up\n",
    "\n",
    "We will need to remove references to `vart.Runner` and let Python garbage-collect\n",
    "the unused graph objects. This will make sure we can run other notebooks without\n",
    "any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del overlay\n",
    "del dpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
